# Copyright The Caikit Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# User can set comma-separated string of config file locations to read
config_files: ""
# merge_strategy can be either `merge` which will merge lists and dicts,
# or `override` which will set the values directly
merge_strategy: "merge"

# Global config switch for runtime check functions (type_check, value_check, file_check...)
enable_error_checks: true
# Maximum number of times that a single exception is logged
max_exception_log_messages: 4

# Configuration for managing the lifecycle of models
model_management:
    # How models are trained through the ModelManager. Each section is a key/val
    # mapping from a user-defined label to an object with a "type" and
    # (optional) "config" key. The "default" key is treated as a special entry
    # that is used if no version is given explicitly at train() or load() time.
    trainers:
        default:
            type: LOCAL
            # import_class can be used to import loaders from third-party packages
            import_class: null
            config:
                # Format: XdXhXmXs
                retention_duration: 1d
                use_subprocess: false
                subprocess_start_method: spawn
    finders:
        default:
            type: LOCAL
            config:
                load_path: null
    initializers:
        default:
            type: LOCAL
            config:
                # List of module backend configurations in priority order
                backend_priority:
                    - type: LOCAL

log:
    # Default level for all python loggers
    level: info
    # Alog filter customization [comma separated list of channels and the level to set them to]
    filters: botocore:off,urllib3:off,matplotlib:off,boto3:off,jnius.reflect:off
    # Width of channels in ALOG when using the pretty formatter
    channel_width: 12
    # if bool(int(ALOG_THREAD_ID)), then ALOG will also log information about thread IDs
    thread_id: true
    # Which formatter to use. options are 'json' or 'pretty'
    formatter: json

# Configuration for data streams and how they're handled in the server
data_streams:
    # Base directory where stream source relative paths should be found
    file_source_base: null

    # Config for data stream source plugins. The keys in the map are names for
    # environment scoping and the values are factory blobs for an importable
    # factory. Each blob requires a `type` and can optionally have `config` and
    # `import_class`.
    source_plugins:
        inline:
            type: JsonData
        file:
            type: FileData
        file_list:
            type: ListOfFiles
        directory:
            type: Directory
        s3:
            type: S3Files

### Runtime configurations
runtime:
    # The runtime library (or libraries) whose models we want to serve using Caikit Runtime. This should
    # be a snake case string, e.g., caikit_nlp or caikit_cv.
    library: sample_lib # TODO: replace with libraries below when runtime can support multiple libraries
    local_models_dir: models

    # If enabled, the models in local_models_dir will be periodically sync'ed
    # with the in-memory models. New models that are not in-memory that are
    # found in local_models_dir will be loaded and existing models that were
    # previously loaded from local_models_dir, but are no longer present will be
    # unloaded.
    lazy_load_local_models: false
    lazy_load_poll_period_seconds: 10
    # Number of times to retry on lazy load failure. This mitigates lazy loads
    # that fail due to partially uploaded model artifacts when the load is
    # initiated.
    lazy_load_retries: 2

    # Number of threads to make available for load jobs (null => all)
    load_threads: null

    # TLS configs
    tls:
        server:
            key: ""
            cert: ""
        client:
            cert: ""

    # Configuration for the grpc server
    grpc:
        enabled: true
        port: 8085
        # Number of workers with which we will run the gRPC server
        server_thread_pool_size: 5
        # gRPC Server shutdown grace period
        # the server shuts down immediately when stopped,
        # and all RPCs active at the end of the grace period are aborted
        server_shutdown_grace_period_seconds: 45
        # Listen on a unix socket (model mesh feature)
        unix_socket_path: /tmp/mmesh/grpc.sock
        # Additional server options as key/value pairs
        # CITE: https://github.com/grpc/grpc/blob/master/include/grpc/impl/channel_arg_names.h#L22
        options: {}

    # Configuration for the http server
    http:
        enabled: true
        port: 8080
        # String prefix for API routes
        route_prefix: api/v1
        # Maximum number of seconds to wait for graceful shutdown
        server_shutdown_grace_period_seconds: 5

    # Configuration for the metrics server
    metrics:
        enabled: true
        port: 8086

    # Whether or not to abort work on client cancellation
    use_abortable_threads: true

    # Configuration items for service generation
    service_generation:
        # Toggles for the generated inference and training services
        enable_inference: true
        enable_training: true

        # Configurations for which module implementations and tasks to include
        # in the service generation
        module_guids:
            included: []
            excluded: []
        task_types:
            included: []
            excluded: []
        backwards_compatibility:
            enabled: false
            current_modules_path: modules.json
            prev_modules_path: prev_modules.json

    # Configuration for batch inference. This dict contains entries for individual
    # model types and can be extended using additional overlay config files to add
    # sections for model types beyond those listed here. Each section should contain
    # the `size` value where a 0 indicates no batching and may optionally contain a
    # `collect_delay_s` value as a float number of seconds >= 0. This will indicate
    # the amount of time the batch should wait for more requests before running a
    # partial batch.
    batching:
        default:
            size: 0
            collect_delay_s: 0.0
        fake_batch_module:
            size: 10

    metering:
        # Switch off metering by default
        enabled: false
        # Directory to save metrics files
        log_dir: "metering_logs"
        # Write to log file every N seconds
        log_interval: 3600

    # Training configuration for server
    training:
        # The directory to save trained models in
        output_dir: training_output
        save_with_id: true
        model_saver_plugins:
            local:
                type: Local
                config:
                    save_with_id: true # Could be top level for all model saver plugins

    # Version details to retrieve
    version_info:
        python_packages:
            all: false
        runtime_image: ""

inference_plugin:
    model_mesh:
        # Model Mesh specific versions
        runtime_version: real
        numeric_runtime_version: 0

        # Model Mesh capacity in bytes for holding models (1024 ^ 3)
        capacity: 10737418240
        max_loading_concurrency: 2
        # Should be big enough to load any really chonky models
        model_loading_timeout_ms: 121000
        # Whether or not model-mesh should respect MAX_MODEL_CONCURRENCY and scale out model loads appropriately
        latency_based_autoscaling_enabled: true
        # If latency based autoscaling is enabled, the default max in-flight requests per copy of each model
        max_model_concurrency: 2
        # ...which can also be set per-model-type
        max_model_concurrency_per_type:
            keywords_text-rank: 8

        # Model Mesh's Default model size in bytes. This needs to be a
        # certain size, or else Model Mesh won't come up correctly (18 * 1024 ^ 2)
        default_model_size: 18874368
        default_model_size_multiplier: 10
        model_size_multipliers:
            # NOTE: This won't work until https://github.com/caikit/caikit/issues/37 is addressed
            categories_esa: 2.95
            concepts_alchemy: 13
            entities_alchemy-disambig: 13.76
            entity-mentions_bilstm: 1.44
            entity-mentions_rbr: 1.8
            sentiment_cnn: 2.73
            syntax_izumo: 43000
            fake_module: 2

### Libraries configuration
libraries:
    sample_lib:
        # Version override for dev running
        version: 1.2.3
        # Module path override if nested
        module_path: sample_lib
